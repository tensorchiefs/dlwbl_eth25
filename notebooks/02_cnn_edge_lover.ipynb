{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K8Ug6ICkRtQ"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorchiefs/dlwbl_eth25/blob/master/notebooks/02_cnn_edge_lover.ipynb)\n",
    "\n",
    "# A simple CNN for the edge lover task\n",
    "**Storytime:** There is an artlover who only likes images with vertical stripes. The goal of this notebook is to use an algorithm (CNN) which helps to classify if the art lover likes the image or not.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_course_2024/refs/heads/main/notebooks/art_lover.png\" alt=\"Sample Image\" width=\"150\">\n",
    "\n",
    "**Task:**\n",
    "You train a very simple CNN with only 1 kernel to distinguish between images containing vertical and images containing horizontal stripes. To check what pattern is recognized by the learned kernel you will visualize the weights of the kernel as an image. You will see that the CNN learns a useful kernel (either a vertical or horiziontal bar). You can experiment with the code to check the influence of the kernel size, the activation function and the pooling method on the result.  \n",
    "\n",
    "\n",
    "**Dataset:** You work with an artficially generated dataset of greyscale images (50x50 pixel) with 10 vertical or horizontal bars. We want to classify them into whether the art lover, who only loves vertical strips, will like the image (y = 0) or not like the image (y = 1).  \n",
    "\n",
    "The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it.  \n",
    "\n",
    "**Content:**\n",
    "* definig and generating the dataset X_train and X_val\n",
    "* visualize samples of the generated images\n",
    "* use keras to train a CNN with only one kernel (5x5 pixel)\n",
    "* visualize the weights of the learned kernel and interpret if it is useful\n",
    "* repeat the last two steps to check if the learned kernel is always the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiB8bJNYn8oP"
   },
   "source": [
    "### Imports\n",
    "\n",
    "In the next cell, we load all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2PDLAWRQ7iUB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras_version: 3.8.0\n",
      "torch_version: 2.5.1\n",
      "keras backend: torch\n"
     ]
    }
   ],
   "source": [
    "# load required libraries:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] =\"torch\"\n",
    "import keras\n",
    "import torch\n",
    "print(f'Keras_version: {keras.__version__}')# 3.5.0\n",
    "print(f'torch_version: {torch.__version__}')# 2.5.1+cu121\n",
    "print(f'keras backend: {keras.backend.backend()}')\n",
    "\n",
    "# Keras Building blocks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq0FNqcBpj23"
   },
   "source": [
    "### Defining functions to generate images\n",
    "\n",
    "Here we define the function to genere images with vertical and horizontal bars, the arguments of the functions are the size of the image and the number of bars you want to have. The bars are at random positions in the image with a random length. The image is black and white, meaning we have only two values for the pixels, 0 for black and 255 for white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nqVBlR8yAO9c"
   },
   "outputs": [],
   "source": [
    "#define function to generate image with shape (size, size, 1) with stripes\n",
    "def generate_image_with_bars(size, bar_nr, vertical = True):\n",
    "    img = np.zeros((size,size,1), dtype=\"uint8\")\n",
    "    for i in range(0,bar_nr):\n",
    "        x,y = np.random.randint(0,size,2)\n",
    "        l  = int(np.random.randint(y,size,1)[0])\n",
    "        if (vertical):\n",
    "            img[y:l,x,0]=255\n",
    "        else:\n",
    "            img[x,y:l,0]=255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUmdGzQLdqzB"
   },
   "source": [
    "Let's have a look at the generated images. We choose a size of 50x50 pixels and set the number of bars in the image to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EccLz0FlXGuU"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# have a look on two generated images\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(1,2,1)\n",
    "img=generate_image_with_bars(50,10, vertical=True)\n",
    "plt.imshow(img[:,:,0],cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "img=generate_image_with_bars(50,10, vertical=False)\n",
    "plt.imshow(img[:,:,0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8gSwmyaevTk"
   },
   "source": [
    "### Make a train and validation dataset of images with vertical and horizontal images\n",
    "Now, let's make a train dataset *X_train* with 1000 images (500 images with vertical and 500 images with horizontal bars). We normalize the images values to be between 0 and 1 by dividing all values with 255. We create a secont dataste *X_val* with exactly the same properties to validate the training of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "63omuptEILKu"
   },
   "outputs": [],
   "source": [
    "pixel=50  # define height and width of images\n",
    "num_images_train = 1000 #Number of training examples (divisible by 2)\n",
    "num_images_val = 1000 #Number of training examples (divisible by 2)\n",
    "\n",
    "# generate training data with vertical edges\n",
    "X_train =np.zeros((num_images_train,pixel,pixel,1))\n",
    "for i in range(0, num_images_train//2):\n",
    "    X_train[i]=generate_image_with_bars(pixel,10)\n",
    "# ... with horizontal\n",
    "for i in range(num_images_train//2, num_images_train):\n",
    "    X_train[i]=generate_image_with_bars(pixel,10, vertical=False)\n",
    "\n",
    "# generate validation data with vertical edges\n",
    "X_val =np.zeros((num_images_train,pixel,pixel,1))\n",
    "for i in range(0, num_images_train//2):\n",
    "    X_val[i]=generate_image_with_bars(pixel,10)\n",
    "# ... with horizontal\n",
    "for i in range(num_images_train//2, num_images_train):\n",
    "    X_val[i]=generate_image_with_bars(pixel,10, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kvAEj2e4xIoK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50, 50, 1)  min  0.0  max  1.0\n",
      "(1000, 50, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "# normalize the data to be between 0 and 1\n",
    "X_train=X_train/255\n",
    "X_val=X_val/255\n",
    "\n",
    "print(X_train.shape, \" min \", np.min(X_train), \" max \", np.max(X_train))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajNnUoYyi7IQ"
   },
   "source": [
    "Here we make the labels for the art lover, 0 means he likes the image (vertical bars) and 1 means that he doesn't like it (horizontal stripes). We one hot encode the labels because we want to use two outputs in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "41-L5hM8S_ZP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)  min  0  max  1\n"
     ]
    }
   ],
   "source": [
    "# create class labels\n",
    "y = np.array([[0],[1]])\n",
    "Y_train = np.repeat(y, num_images_train //2)\n",
    "Y_val = np.repeat(y, num_images_train //2)\n",
    "\n",
    "# one-hot-encoding\n",
    "#Y_train = to_categorical(Y_train,2)\n",
    "#Y_val = to_categorical(Y_val,2)\n",
    "print(Y_train.shape, \" min \", np.min(Y_train), \" max \", np.max(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZpr0h-VvatF"
   },
   "source": [
    "## Defining the CNN\n",
    "\n",
    "Here we define the CNN:\n",
    "\n",
    "- we use only one kernel with a size of 5x5 pixels  \n",
    "- then we apply a linar activation function  \n",
    "- the maxpooling layer takes the maximum of the whole activation map to predict the probability (output layer with sigmoid) if the art lover will like the image\n",
    "\n",
    "As loss we use the binary_crossentropy and we train the model with a batchsize of 64 images per update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1Dfg1h2rUifd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oli/miniconda3/envs/dlwbl_eth25_env/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(1,(5,5),padding='same',input_shape=(pixel,pixel,1)))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "# take the max over all values in the activation map\n",
    "model.add(MaxPooling2D(pool_size=(pixel,pixel)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# compile model and initialize weights\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "r6eqV0TRU0_n"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │            \u001b[38;5;34m26\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m2\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28</span> (112.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28\u001b[0m (112.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28</span> (112.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28\u001b[0m (112.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's summarize the CNN architectures along with the number of model weights\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Sc-BYd8kVCx0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.4943 - loss: 0.8140 - val_accuracy: 0.5000 - val_loss: 0.7571\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5229 - loss: 0.7255 - val_accuracy: 0.5000 - val_loss: 0.7081\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5130 - loss: 0.6903 - val_accuracy: 0.5000 - val_loss: 0.6714\n",
      "Epoch 4/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5036 - loss: 0.6655 - val_accuracy: 0.5000 - val_loss: 0.6425\n",
      "Epoch 5/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4931 - loss: 0.6487 - val_accuracy: 0.5000 - val_loss: 0.6204\n",
      "Epoch 6/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5168 - loss: 0.6047 - val_accuracy: 0.5000 - val_loss: 0.6003\n",
      "Epoch 7/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5093 - loss: 0.5943 - val_accuracy: 0.5000 - val_loss: 0.5810\n",
      "Epoch 8/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5173 - loss: 0.5669 - val_accuracy: 0.5000 - val_loss: 0.5628\n",
      "Epoch 9/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4999 - loss: 0.5603 - val_accuracy: 0.5000 - val_loss: 0.5454\n",
      "Epoch 10/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4951 - loss: 0.5443 - val_accuracy: 0.5000 - val_loss: 0.5287\n",
      "Epoch 11/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5281 - loss: 0.5199 - val_accuracy: 0.5700 - val_loss: 0.5135\n",
      "Epoch 12/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5874 - loss: 0.5013 - val_accuracy: 0.6410 - val_loss: 0.4991\n",
      "Epoch 13/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6524 - loss: 0.4864 - val_accuracy: 0.6570 - val_loss: 0.4857\n",
      "Epoch 14/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7226 - loss: 0.4731 - val_accuracy: 0.8420 - val_loss: 0.4727\n",
      "Epoch 15/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8653 - loss: 0.4660 - val_accuracy: 0.8720 - val_loss: 0.4602\n",
      "Epoch 16/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8563 - loss: 0.4534 - val_accuracy: 0.9170 - val_loss: 0.4463\n",
      "Epoch 17/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9289 - loss: 0.4374 - val_accuracy: 0.9520 - val_loss: 0.4311\n",
      "Epoch 18/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9631 - loss: 0.4264 - val_accuracy: 0.9700 - val_loss: 0.4173\n",
      "Epoch 19/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9689 - loss: 0.4078 - val_accuracy: 0.9770 - val_loss: 0.4046\n",
      "Epoch 20/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9806 - loss: 0.4047 - val_accuracy: 0.9800 - val_loss: 0.3923\n",
      "Epoch 21/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9855 - loss: 0.3793 - val_accuracy: 0.9880 - val_loss: 0.3790\n",
      "Epoch 22/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9838 - loss: 0.3737 - val_accuracy: 0.9890 - val_loss: 0.3657\n",
      "Epoch 23/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9849 - loss: 0.3541 - val_accuracy: 0.9930 - val_loss: 0.3528\n",
      "Epoch 24/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9948 - loss: 0.3458 - val_accuracy: 0.9960 - val_loss: 0.3406\n",
      "Epoch 25/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9973 - loss: 0.3419 - val_accuracy: 0.9990 - val_loss: 0.3292\n",
      "Epoch 26/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9978 - loss: 0.3301 - val_accuracy: 0.9990 - val_loss: 0.3184\n",
      "Epoch 27/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9996 - loss: 0.3164 - val_accuracy: 0.9990 - val_loss: 0.3079\n",
      "Epoch 28/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9993 - loss: 0.3005 - val_accuracy: 1.0000 - val_loss: 0.2968\n",
      "Epoch 29/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9996 - loss: 0.2938 - val_accuracy: 1.0000 - val_loss: 0.2854\n",
      "Epoch 30/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9978 - loss: 0.2804 - val_accuracy: 1.0000 - val_loss: 0.2745\n",
      "Epoch 31/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9968 - loss: 0.2741 - val_accuracy: 1.0000 - val_loss: 0.2642\n",
      "Epoch 32/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9978 - loss: 0.2627 - val_accuracy: 1.0000 - val_loss: 0.2549\n",
      "Epoch 33/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.2555 - val_accuracy: 1.0000 - val_loss: 0.2460\n",
      "Epoch 34/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9995 - loss: 0.2375 - val_accuracy: 1.0000 - val_loss: 0.2376\n",
      "Epoch 35/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9982 - loss: 0.2366 - val_accuracy: 1.0000 - val_loss: 0.2295\n",
      "Epoch 36/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2333 - val_accuracy: 1.0000 - val_loss: 0.2216\n",
      "Epoch 37/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.2203 - val_accuracy: 1.0000 - val_loss: 0.2142\n",
      "Epoch 38/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2211 - val_accuracy: 1.0000 - val_loss: 0.2071\n",
      "Epoch 39/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2011 - val_accuracy: 1.0000 - val_loss: 0.2002\n",
      "Epoch 40/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1980 - val_accuracy: 1.0000 - val_loss: 0.1936\n",
      "Epoch 41/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1876 - val_accuracy: 1.0000 - val_loss: 0.1873\n",
      "Epoch 42/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1874 - val_accuracy: 1.0000 - val_loss: 0.1812\n",
      "Epoch 43/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1768 - val_accuracy: 1.0000 - val_loss: 0.1755\n",
      "Epoch 44/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1737 - val_accuracy: 1.0000 - val_loss: 0.1700\n",
      "Epoch 45/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1710 - val_accuracy: 1.0000 - val_loss: 0.1646\n",
      "Epoch 46/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1643 - val_accuracy: 1.0000 - val_loss: 0.1594\n",
      "Epoch 47/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1546 - val_accuracy: 1.0000 - val_loss: 0.1545\n",
      "Epoch 48/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1486 - val_accuracy: 1.0000 - val_loss: 0.1498\n",
      "Epoch 49/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1508 - val_accuracy: 1.0000 - val_loss: 0.1451\n",
      "Epoch 50/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1427 - val_accuracy: 1.0000 - val_loss: 0.1407\n",
      "CPU times: user 8 s, sys: 2.05 s, total: 10 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train the model\n",
    "history=model.fit(X_train, Y_train,\n",
    "                  validation_data=(X_val,Y_val),\n",
    "                  batch_size=64,\n",
    "                  epochs=50,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fK_AAAoiQtlc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x322f268f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the development of the accuracy and loss during training\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,(1))\n",
    "plt.plot(history.history['accuracy'],linestyle='-.')\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,(2))\n",
    "plt.plot(history.history['loss'],linestyle='-.')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_n43gBNuv0L"
   },
   "source": [
    "## 🔧 **YOUR TASK:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOwR3Esbw8eN"
   },
   "source": [
    "### Visualize the learned kernel and experiment with the code\n",
    "\n",
    "- You see that the CNN performs very good at this task (100% accuracy). We can check which pattern is recognized by the **learned kernel** and see if you think that this is helpful to distinguish between images with horizontal and vertical edges.\n",
    "\n",
    "- Below you can see the original image, the image after the convolution operation with the learned kernel and the maximum value from the maxpooling operation. Note that the maxpooling has the same size as the convolved image so there is just one value as output.\n",
    "\n",
    "\n",
    "- Move the sliders to inspect different pictures from the validation set and their predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pl1yuAddVRnE"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# @title Random Kernel and Kernel learned based on the data { display-mode: \"form\" }\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## Do not worry about this cell, just move the sliders.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m block_reduce  \u001b[38;5;66;03m# For max pooling\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwidgets\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "# @title Random Kernel and Kernel learned based on the data { display-mode: \"form\" }\n",
    "## Do not worry about this cell, just move the sliders.\n",
    "import scipy.signal\n",
    "from skimage.measure import block_reduce  # For max pooling\n",
    "import ipywidgets as widgets\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.random.rand(25).reshape(5, 5),\"gray\") ,plt.title('Randomly initalized weights')\n",
    "plt.subplot(1, 2, 2)\n",
    "conv_filter=np.squeeze(model.get_weights()[0], axis=2)\n",
    "plt.imshow(conv_filter[:,:,0],\"gray\"),plt.title('Learned Kernel (weights) , by model'),plt.show();\n",
    "\n",
    "def scale_convolution_map(conv_map, min_val=-3, max_val=3):\n",
    "    clipped_conv_map = np.clip(conv_map, min_val, max_val)\n",
    "    scaled_conv_map = (clipped_conv_map - min_val) / (max_val - min_val)\n",
    "    return scaled_conv_map\n",
    "\n",
    "def plot_conv(img,category):\n",
    "  convolved_image = scipy.signal.convolve2d(img.squeeze(), conv_filter.squeeze(), mode='same')\n",
    "  scaled_conv_image = scale_convolution_map(convolved_image + model.get_weights()[1])\n",
    "  max_pooled_image = block_reduce(convolved_image + model.get_weights()[1], block_size=(50, 50), func=np.max)\n",
    "  scaled_max_pooled_image = scale_convolution_map(max_pooled_image)\n",
    "\n",
    "  plt.figure(figsize=(20, 4))\n",
    "  plt.subplot(1, 6, 1)\n",
    "  plt.imshow(img, \"gray\", vmin=0, vmax=1),plt.title('Original Image')\n",
    "  plt.subplot(1, 6, 2)\n",
    "  plt.imshow(scaled_conv_image, \"gray\", vmin=0, vmax=1),plt.title('Convolved Image')\n",
    "  plt.subplot(1, 6, 3),plt.imshow(scaled_max_pooled_image, \"gray\", vmin=0, vmax=1)\n",
    "  plt.title(f'Max Pooled = {max_pooled_image[0][0]:.2f}'),plt.xticks([]), plt.yticks([])\n",
    "  plt.subplot(1, 6, 4),plt.axis('off')\n",
    "  pred = model.predict(img.reshape(1, 50, 50, 1), verbose=0)\n",
    "  if category==\"vertical\":\n",
    "    text_info = f'''\n",
    "    P(y=vertical|x): {pred[0][0]:.4f}\n",
    "    P(y=horizontal|x): {pred[0][1]:.4f}\n",
    "\n",
    "\n",
    "    loss component:  {-np.log(pred[0][0]):.4f}\n",
    "      '''\n",
    "  else:\n",
    "        text_info = f'''\n",
    "    P(y=vertical|x): {pred[0][0]:.4f}\n",
    "    P(y=horizontal|x): {pred[0][1]:.4f}\n",
    "\n",
    "\n",
    "    loss component:  {-np.log(pred[0][1]):.4f}\n",
    "      '''\n",
    "\n",
    "  plt.text(0, 0.5, text_info, ha='left', va='center')\n",
    "  plt.subplot(1, 6, 5)\n",
    "  x_values = np.linspace(0.001, 1.1, 500)\n",
    "  plt.plot(x_values, -np.log(x_values), label='-log(P(y|x))')\n",
    "  plt.ylim(-0.5, 6),plt.xlim(-0.1, 1.1),plt.xlabel('P(y|x)')\n",
    "  if category==\"vertical\":\n",
    "    plt.plot(pred[0][0], -np.log(pred[0][0]), 'bo', label='loss component')\n",
    "  else:\n",
    "    plt.plot(pred[0][1], -np.log(pred[0][1]), 'ro', label='loss component')\n",
    "\n",
    "  plt.legend(),plt.grid(True), plt.tight_layout(),plt.show()\n",
    "\n",
    "def inspect_preds(idx,category='vertical'):\n",
    "  plot_conv(X_val[idx,:,:,0],category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1vicISRzwJ14"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'widgets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m horizontal_slider \u001b[38;5;241m=\u001b[39m \u001b[43mwidgets\u001b[49m\u001b[38;5;241m.\u001b[39mIntSlider(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mnum_images_val\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m widgets\u001b[38;5;241m.\u001b[39minteract(inspect_preds, idx\u001b[38;5;241m=\u001b[39mhorizontal_slider,category\u001b[38;5;241m=\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mfixed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'widgets' is not defined"
     ]
    }
   ],
   "source": [
    "horizontal_slider = widgets.IntSlider(min=0, max=num_images_val//2-1, step=1, value=0, description='vertical')\n",
    "widgets.interact(inspect_preds, idx=horizontal_slider,category=widgets.fixed('vertical'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QaqpNT8wJ14"
   },
   "outputs": [],
   "source": [
    "vertical_slider = widgets.IntSlider(min=num_images_val//2, max=num_images_val-1, step=1, value=0, description='horizontal')\n",
    "widgets.interact(inspect_preds, idx=vertical_slider,category=widgets.fixed('horizontal'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPYFGwP6wJ15"
   },
   "source": [
    "### Excercise 1\n",
    "\n",
    "Given the probabilities, calculate the loss component for one example of your choice from the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p-doVHJ-3rp"
   },
   "source": [
    "<details>\n",
    "  <summary>🔑 Click here to View Answers:</summary>\n",
    "\n",
    "assumed that we have a vertical image,  label = *vertical* or translated to numbers label = *0*\n",
    "\n",
    "assume that probabilites by the model for said example are:\n",
    "\n",
    " $p_{model}(y=0|image\n",
    ") = 0.9$\n",
    "\n",
    "\n",
    "The NLL for a sample is given by: $ l_{i}= -log(p_{model}(y_{i}|x_{i})) $\n",
    "\n",
    "The loss: $loss = \\frac{1}{N} \\sum_{}{} li$\n",
    "\n",
    "\n",
    "\n",
    "$ = - \\left[ \\log(0.9) \\right]$\n",
    "\n",
    "$ =  0.1053 $\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4gnnlAPp_Q2"
   },
   "source": [
    "### Excercise 2\n",
    "\n",
    "\n",
    "- Repeat the compiling and training, beginning from the cell:\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "       \n",
    "       ...\n",
    "       \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "For several times and check if the CNN.\n",
    "\n",
    "Does the CNN always learn the same kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g56ibx1e9QBj"
   },
   "source": [
    "<details>\n",
    "  <summary>🔑 Click here to View Answers:</summary>\n",
    "- No it does not, sometimes it learns the horizontal patterns, and sometimes the vertical pattern.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJC2deYu8vrE"
   },
   "source": [
    "### Additional Excercise\n",
    "\n",
    "- You can experiment with the code and check what happens if you use another kernel size, Number of Kernels,  activation function (relu instead of linear ) or pooling method AveragePooling instead of MaxPooling.  Try to make a prediction on the performance before doing the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeahLP9NBYYo"
   },
   "source": [
    "<details>\n",
    "  <summary>🔑 Click here to View Answers:</summary>\n",
    "- if a second kernels is applied the loss converges much faster!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YwThvI9QzzM"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(2,(5,5),padding='same',input_shape=(pixel,pixel,1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# take the max over all values in the activation map\n",
    "model.add(MaxPooling2D(pool_size=(pixel,pixel)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile model and initialize weights\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# train the model\n",
    "history=model.fit(X_train, Y_train,\n",
    "                  validation_data=(X_val,Y_val),\n",
    "                  batch_size=64,\n",
    "                  epochs=150,\n",
    "                  verbose=1)\n",
    "\n",
    "# plot the development of the accuracy and loss during training\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,(1))\n",
    "plt.plot(history.history['accuracy'],linestyle='-.')\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,(2))\n",
    "plt.plot(history.history['loss'],linestyle='-.')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "kJC2deYu8vrE"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
